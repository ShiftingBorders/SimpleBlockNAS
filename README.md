# SimpleBlockNAS ğŸ§¬ğŸ”

**SimpleBlockNAS** is a *Neural Architecture Search* (NAS) framework that uses *Genetic Algorithms* (GA) to automatically discover optimal neural network architectures and tune hyperparameters.  
It provides a modular and extensible system for automated deep learning model design.

> âš ï¸ **Project Status:** PAUSED/MAINTENANCE.  

---

## ğŸš€ Features

- **Genetic Algorithm-based NAS** â€” evolutionary search for neural architectures
- **Modular Block System** â€” pre-defined building blocks (Linear, Convolution2D, MaxPool2D, Flatten)
- **Flexible Search Settings** â€” configurable mutation and crossover operators
- **Multi-Dataset Support** â€” built-in support for EuroSAT and CIFAR10, easily extendable
- **PyTorch / PyTorch Lightning Integration**
- **Customizable Training Parameters** â€” learning rate, batch size, GA parameters
- **Model Weight Persistence** â€” save and reuse trained model weights

---

## ğŸ— Project Architecture

- **GA Pipeline** â€” mutation, crossover, and selection implementation
- **Block System** â€” modular neural network building blocks
- **Population Management** â€” population handling and evolution
- **Network Validation** â€” architecture verification and parameter alignment
- **Training Pipeline** â€” training and evaluation

---

## ğŸ“¦ Installation

### Requirements

- Python 3.8+
- PyTorch 2.4.1+
- Torchvision 0.19.1+
- Torchmetrics 1.4.2+
- CUDA (optional, for GPU acceleration)

### Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Quick Start

Example run is available in:
```
experiments.py
```

---

## ğŸ§© Available Blocks

**Basic:**
- `Linear` â€” fully connected layers (activation, dropout)
- `Convolution2d` â€” 2D convolutional layers with batch norm and dropout
- `MaxPool2d` â€” max pooling
- `Flatten` â€” tensor flattening

**Configuration Features:**
- Custom activation functions (ReLU, Softmax, None)
- Dropout layers
- Batch normalization
- Parameter randomization
- Custom connections

---

## ğŸ“Š Logging & Results

The system automatically saves:
- Evolution progress
- Training metrics and validation scores
- Population statistics
- All evaluated architectures

Reports are stored in the `reports/` folder with timestamps and experiment names.

---

## ğŸ¤ Contributing

If you have questions or suggestions, feel free to open an issue on GitHub or contact me directly.

---

## ğŸ“„ License

MIT License â€” see the [LICENSE](LICENSE) file for details.

---
